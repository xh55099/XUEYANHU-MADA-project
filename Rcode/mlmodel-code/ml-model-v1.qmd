---
title: "An example exploratory analysis script"
author: "Xueyan Hu"
date: "03/22/2024"
output: html_document
---

# Setup

```{r}
#load needed packages. make sure they are installed.
library(here) #for data loading/saving
library(dplyr)
library(skimr)
library(ggplot2)
library(tidymodels)
library(discrim)
library(randomForest)
library(caret)
library(lattice)
library(tidymodels)
library(ranger)
library(parsnip)
library(glmnet)
library(MASS)
library(nnet)
```

Load the data.

```{r}
#Path to data. Note the use of the here() package and not absolute paths
data_location <- here::here("data","processed-data","processeddata.rds")
#load data
mydata <- readRDS(data_location)
```

# fit simple linear model

First, I am curious that if physical activity could be correlated to the choice of transportation since I have heard a theory that if someone take much effort on commute transportation, they tend to spend less time on exercise. First I want to use number to represent the type of transportation. Walking requires most energy consumption, then it goes with public transportation and automobile is the least.

```{r}
# Define a function to map transportation modes to numeric values
map_transport <- function(transport_mode) {
  if (transport_mode == "Walking") {
    return(4)
  } else if (transport_mode == "Motorbike") {
    return(2)
  } else if (transport_mode == "Public_Transportation") {
    return(3)
  } else if (transport_mode == "Automobile") {
    return(1)
  } else {
    return(0)  # Return 0 for other cases
  }
}

# Apply the function to create the new variable
mydata$MTRANSn <- sapply(mydata$MTRANS, map_transport)

# Calculate correlation coefficient
correlation <- cor(mydata$FAF, mydata$MTRANSn)

# Print correlation coefficient
print(correlation)
```

Since the correlation coefficient is close to zero, there is almost no linear association between them. So this result doesn't really support the theory that I have heard from social media.

Then, I would like to see how drinking water plus physical activity affect obesity.

```{r}
# linear model of BMI by water and FAF
lm1 <- lm(BMI ~ Water + FAF + Water * FAF, data = mydata)
lm1_result <- summary(lm1)

# save the result
lm_file = here("results", "tables", "lm1table.rds")
saveRDS(lm1_result, file = lm_file)
```

In addition, I want to fit model for alcohol consumption and obesity level.

```{r}
mydata$Alcohol <- as.factor(mydata$Alcohol)
lm2 <- lm(BMI ~ Alcohol, data = mydata)
lm2_result <- summary(lm2)

# save the result
lm_file = here("results", "tables", "lm2table.rds")
saveRDS(lm2_result, file = lm_file)
```

I think it makes a little bit sense to me that there is a connection effect of water and physical activity on BMI, which might be supportive for what we learn that drinking more water and doing more exercise will lower the risk of obesity.

I haven't include gender or age for model fitting, I will try to combine them in one model here.

```{r}
lm4 <- lm(BMI ~ Gender * Age, data = mydata)
lm4_result <- summary(lm4)

# save the result
lm_file = here("results", "tables", "lm4table.rds")
saveRDS(lm4_result, file = lm_file)
```

This linear model proves gender and age can also be predictors for BMI and there is interaction between them. So I will include them in the all predictor model.

Last, I will try to use all predictors that I believe may affect BMI to fit a model.

```{r}
lm3 <- lm(BMI ~ Gender + Age + History + Water + Alcohol + FAF + MTRANS, data = mydata)
lm3_result <- summary(lm3)

# save the result
lm_file = here("results", "tables", "lm3table.rds")
saveRDS(lm3_result, file = lm_file)
```

Choose predictors only important for the model fitting. Thanks to my classmates' replies, I received some useful suggestion for choosing predictors, which are chi-square, LASSO and lowest RMSE. According to my searching result online, chi-square is good for categorical variable as outcome, and LASSO might shrink the coefficients too much. So I will go with the lowest RMSE in the following and see what will happen next.

```{r}
# Filter out observations where Alcohol is 4
mydata1 <- mydata[mydata$Alcohol != 4, ]
# Split data into training and testing sets
set.seed(123)  # Set seed for reproducibility
trainIndex <- createDataPartition(mydata$BMI, p = 0.8, list = FALSE)
trainData <- mydata1[trainIndex, ]
testData <- mydata1[-trainIndex, ]
# Create indices for 10-fold cross-validation
cv_folds <- createFolds(trainData$BMI, k = 10)
```

Since there is only one observation with alcohol consumption level at 4, so after splitting the data, it will be distributed randomly in either trainData or testData. When it is in trainData and the cross-validation is applied, there will be some folds doesn't have this level and error will occur during this process. So I drop it from the dataset to make it working easier.

I will fit the first linear model which is called linear model 1 to the data and use cross-validation, then calculate RMSE. There is no interactions in this model's predictors. And by running this chunk, predictor combination that indicates the lowest RMSE will be selected and shown in the output together with its RMSE.

```{r}
# predictors excluding interactions first
# Define predictors
predictors <- c("Gender", "Age", "History", "Water", "Alcohol", "FAF", "MTRANS")

# Set seed
set.seed(123)

# Iterate through all possible combinations of predictors
for (i in 1:length(predictors)) {
  subsets1 <- combn(predictors, i)
  for (j in 1:ncol(subsets1)) {
    predictors_subset1 <- subsets1[, j]
    
    # Initialize variable to store RMSE for current combination of predictors
    total_rmse_lm1 <- 0
    
    # Perform cross-validation
    for (fold in 1:10) {
      # Extract indices for training and validation data for current fold
      train_indices <- unlist(cv_folds[-fold])
      validation_indices <- cv_folds[[fold]]
      
      # Fit model using subset of predictors
      formula_lm1 <- paste("BMI ~", paste(predictors_subset1, collapse = " + "))
      model_lm1 <- lm(formula_lm1, data = trainData[train_indices, ])
      
      # Make predictions on validation set
      predictions_lm1 <- predict(model_lm1, newdata = trainData[validation_indices, ])
      
      # Calculate RMSE for current fold
      fold_rmse_lm1 <- sqrt(mean((trainData$BMI[validation_indices] - predictions_lm1)^2))
      
      # Add RMSE for current fold to total RMSE
      total_rmse_lm1 <- total_rmse_lm1 + fold_rmse_lm1
    }
    
    # Calculate average RMSE across all folds
    avg_rmse_lm1 <- total_rmse_lm1 / 10
    
    # Update minRMSE and bestSubset if average RMSE is lower
    if (avg_rmse_lm1 < minRMSE1) {
      minRMSE1 <- avg_rmse_lm1
      bestSubset1 <- predictors_subset1
    }
  }
}

# Print results
print(paste("Best subset of predictors:", paste(bestSubset1, collapse = ", ")))
print(paste("Lowest RMSE:", minRMSE1))
```

Then I will calculate RMSE for null model of linear regression.

```{r}
# calculate null model RMSE
# Initialize variable to store total RMSE across all folds
total_rmse_null <- 0

# Perform cross-validation
for (fold in 1:10) {
  # Extract indices for training and validation data for current fold
  train_indices <- unlist(cv_folds[-fold])
  validation_indices <- cv_folds[[fold]]
  
  # Compute the mean BMI from the training data
  mean_bmi_train <- mean(trainData$BMI[train_indices])
  
  # Create a vector of predicted values equal to the mean BMI from training data
  predicted_bmi_validation <- rep(mean_bmi_train, length(validation_indices))
  
  # Compute RMSE (Root Mean Squared Error) on validation data for current fold
  rmse_null_validation <- sqrt(mean((trainData$BMI[validation_indices] - predicted_bmi_validation)^2))
  
  # Add RMSE for current fold to total RMSE
  total_rmse_null <- total_rmse_null + rmse_null_validation
}

# Calculate average RMSE across all folds
avg_rmse_null <- total_rmse_null / 10

# Print the results
print(paste("Null Model RMSE using Cross-Validation:", avg_rmse_null))
```

Using the same linear regression named linear model 2 but adding 2 interactions as predictors and under these predictors, best combination with lowest RMSE will be shown in the output.

```{r}
# interactions included in predictors
# Define predictors and response variable
predictors <- c("Gender", "Age", "History", "Water", "Alcohol", "FAF", "MTRANS")
interaction_terms <- c("Gender_Age", "FAF_Water")  # Define interaction terms
response <- "BMI"

# Create interaction terms in training and testing data
trainData$Gender_Age <- trainData$Gender * trainData$Age
trainData$FAF_Water <- trainData$FAF * trainData$Water

testData$Gender_Age <- testData$Gender * testData$Age
testData$FAF_Water <- testData$FAF * testData$Water

# Update list of predictors to include interaction terms
predictors_updated <- c(predictors, "Gender_Age", "FAF_Water")

# Initialize variables to store results
minRMSE2 <- Inf
bestSubset2 <- NULL

# Set seed
set.seed(123)

# Iterate through all possible combinations of predictors
for (i in 1:length(predictors_updated)) {
  subsets2 <- combn(predictors_updated, i)
  for (j in 1:ncol(subsets2)) {
    predictors_subset2 <- subsets2[, j]
    
    # Initialize variable to store RMSE for current combination of predictors
    total_rmse_lm2 <- 0
    
    # Perform cross-validation
    for (fold in 1:10) {
      # Extract indices for training and validation data for current fold
      train_indices <- unlist(cv_folds[-fold])
      validation_indices <- cv_folds[[fold]]
      
      # Fit model using subset of predictors
      formula_lm2 <- paste("BMI ~", paste(predictors_subset2, collapse = " + "))
      model_lm2 <- lm(formula_lm2, data = trainData[train_indices, ])
      
      # Make predictions on validation set
      predictions_lm2 <- predict(model_lm2, newdata = trainData[validation_indices, ])
      
      # Calculate RMSE for current fold
      fold_rmse_lm2 <- sqrt(mean((trainData$BMI[validation_indices] - predictions_lm2)^2))
      
      # Add RMSE for current fold to total RMSE
      total_rmse_lm2 <- total_rmse_lm2 + fold_rmse_lm2
    }
    
    # Calculate average RMSE across all folds
    avg_rmse_lm2 <- total_rmse_lm2 / 10
    
    # Update minRMSE and bestSubset if average RMSE is lower
    if (avg_rmse_lm2 < minRMSE2) {
      minRMSE2 <- avg_rmse_lm2
      bestSubset2 <- predictors_subset2
    }
  }
}

# Print results
print(paste("Best subset of predictors (including interactions) using cross-validation:", paste(bestSubset2, collapse = ", ")))
print(paste("Lowest RMSE (including interactions) using cross-validation:", minRMSE2))
```

It shows different predictor combinations but the good thing is both of the linear models have lower RMSE than null model which indicates they perform better than null model. Then, I will try LASSO model.

```{r}
# Create dummy variables for categorical variables in trainData
trainData1 <- within(trainData, {
  History <- as.factor(History)
  MTRANS <- as.factor(MTRANS)
})
dummy_vars_train <- model.matrix(~ History + MTRANS - 1, data = trainData1)

# Combine dummy variables with original trainData
trainData1 <- cbind(trainData, dummy_vars_train)

# Repeat the same process for testData
testData1 <- within(testData, {
  History <- as.factor(History)
  MTRANS <- as.factor(MTRANS)
})
dummy_vars_test <- model.matrix(~ History + MTRANS - 1, data = testData1)
testData1 <- cbind(testData, dummy_vars_test)

# Define predictors and outcome
response <- "BMI"
predictors <- c("Gender", "Age", "History", "Water", "Alcohol", "FAF", "MTRANS")
interaction_terms <- c("Gender_Age", "FAF_Water")

# Update predictors to include dummy variables
predictors_lasso <- c("Gender", "Age", "Water", "Alcohol", "FAF","Gender_Age", "FAF_Water", colnames(dummy_vars_train))


# Define function to calculate RMSE
calculate_rmse <- function(predictions, actual) {
  return(sqrt(mean((actual - predictions)^2)))
}

# Initialize variables to store results
min_rmse_lasso <- Inf
best_subset_lasso <- NULL

# Set seed
set.seed(123)

# Perform LASSO regression with cross-validation
for (fold in 1:10) {
  # Extract indices for training and validation data for current fold
  train_indices <- cv_folds[[fold]]
  validation_indices <- setdiff(1:nrow(trainData1), train_indices)
  
  # Define training and validation data subsets
  train_data_fold <- trainData1[train_indices, ]
  validation_data_fold <- trainData1[validation_indices, ]
  
  # Create design matrix for training and validation data
  train_matrix <- model.matrix(~., data = train_data_fold[, predictors_lasso])[,-1]
  validation_matrix <- model.matrix(~., data = validation_data_fold[, predictors_lasso])[,-1]
  
  # Ensure the same variables are present in training and validation matrices
  common_variables <- intersect(colnames(train_matrix), colnames(validation_matrix))
  train_matrix <- train_matrix[, common_variables]
  validation_matrix <- validation_matrix[, common_variables]
  
  # Perform LASSO regression with cross-validation
  lasso_model <- cv.glmnet(train_matrix, train_data_fold$BMI, alpha = 1, lambda = 10^seq(10, -2, length = 100))
  
  # Extract optimal lambda value
  optimal_lambda <- lasso_model$lambda.min
  
  # Fit LASSO model using optimal lambda on training data
  lasso_model_optimal <- glmnet(train_matrix, train_data_fold$BMI, alpha = 1, lambda = optimal_lambda)
  
  # Make predictions on validation set
  predictions_lasso <- predict(lasso_model_optimal, newx = validation_matrix, s = optimal_lambda)
  
  # Calculate RMSE on validation set
  rmse <- sqrt(mean((validation_data_fold$BMI - predictions_lasso)^2))
  
  # Update min_rmse and best_subset if CV RMSE is lower
  if (rmse < min_rmse_lasso) {
    min_rmse_lasso <- rmse
    best_subset_lasso <- predictors_lasso  # Include interaction terms
  }
}

# Extract non-zero coefficients from the optimal LASSO model
non_zero_coeffs <- coef(lasso_model_optimal)
non_zero_coeffs <- non_zero_coeffs[-1]  # Remove intercept

# Find the indices of non-zero coefficients
non_zero_indices <- which(non_zero_coeffs != 0)

# Extract the names of predictors with non-zero coefficients
best_subset_names_lasso <- colnames(train_matrix)[non_zero_indices]

# Extract the optimal lambda value
optimal_lambda <- lasso_model$lambda.min

# Print results
cat("Lowest RMSE (LASSO CV):", min_rmse_lasso, "\n")
cat("Best subset of predictors:", paste(best_subset_names_lasso, collapse = ", "), "\n")
# Print optimal lambda value
cat("Optimal lambda:", optimal_lambda, "\n")
```

Since I already get optimal lambda above, so I will skip using autoplot function creating figure and tuning the LASSO model.

For trying with more models as comparison, I select another regression model to fit data, here I will use random forest model.

```{r}
# Create formula with response and all predictors
predictors_updated <- c(predictors, "Gender_Age", "FAF_Water")
formula_rf <- as.formula(paste(response, "~", paste(predictors_updated, collapse = " + ")))

# Initialize variables to store results
best_rmse_rf <- Inf
best_predictors_rf <- NULL

# Set seed
set.seed(123)

# Forward selection with cross-validation
for (fold in seq_along(cv_folds)) {
  # Extract training and validation indices for current fold
  train_indices <- unlist(cv_folds[-fold])
  validation_indices <- cv_folds[[fold]]
  
  # Initialize variables to store results for current fold
  best_rmse_fold <- Inf
  best_predictors_fold <- NULL
  
  # Forward selection
  for (i in seq_along(predictors_updated)) {
    predictors_subset_rf <- predictors_updated[1:i]
    formula_subset_rf <- as.formula(paste(response, "~", paste(predictors_subset_rf, collapse = " + ")))
    
    # Fit random forest model using the training data for the current fold
    model_rf <- randomForest(formula = formula_subset_rf, data = trainData[train_indices, ])
    
    # Make predictions on the validation data for the current fold
    predictions_rf <- predict(model_rf, newdata = trainData[validation_indices, ])
    
    # Calculate RMSE for the current fold
    rmse_rf <- sqrt(mean((trainData$BMI[validation_indices] - predictions_rf)^2))
    
    # Check if current RMSE is better than the best RMSE so far for this fold
    if (rmse_rf < best_rmse_fold) {
      best_rmse_fold <- rmse_rf
      best_predictors_fold <- predictors_subset_rf
    }
  }
  
  # Check if the best RMSE for this fold is better than the best RMSE so far
  if (best_rmse_fold < best_rmse_rf) {
    best_rmse_rf <- best_rmse_fold
    best_predictors_rf <- best_predictors_fold
  }
}

# Print the best combination of predictors
print("Best combination of predictors for random forest model:")
print(best_predictors_rf)
print(best_rmse_rf)
```

Since the measurement of obesity also can be shown by different degree levels, I will use obesity level as a new outcome and fit logistic models to the dataset so that more possibilities can be listed and more models can be compared.

First, I can use discriminant analysis model to fit the data since here are totally 7 categories classified by the original authors of the dataset.

```{r}
# Define the outcome variable
outcome <- "Obesity"

# Define predictor variables
predictors_updated <- c(predictors, "Gender_Age", "FAF_Water")

# Initialize variables to store results
best_subset_DA <- NULL
best_accuracy_DA <- 0

# Set seed
set.seed(123)

# Create cross-validation folds
cv_fold1 <- createFolds(trainData$Obesity, k = 10)

# Iterate through all possible combinations of predictors
for (i in 1:length(predictors_updated)) {
  subsets <- combn(predictors_updated, i)
  for (j in 1:ncol(subsets)) {
    predictors_subset_DA <- subsets[, j]
    
    # Initialize variable to store accuracy for each fold
    fold_accuracies <- numeric(length(cv_fold1))
    
    # Perform cross-validation
    for (fold_index in seq_along(cv_fold1)) {
      # Extract training and validation indices for current fold
      train_indices <- unlist(cv_fold1[-fold_index])
      validation_indices <- cv_fold1[[fold_index]]
      
      # Fit the DA model using the subset of predictors on the training data for the current fold
      formula_DA <- as.formula(paste(outcome, "~", paste(predictors_subset_DA, collapse = " + ")))
      model_DA <- lda(formula_DA, data = trainData[train_indices, ])
      
      # Make predictions on the validation set
      predicted_DA <- predict(model_DA, newdata = trainData[validation_indices, ])
      
      # Calculate accuracy for the current fold
      fold_accuracies[fold_index] <- mean(predicted_DA$class == trainData$Obesity[validation_indices])
    }
    
    # Calculate average accuracy across all folds
    average_accuracy <- mean(fold_accuracies)
    
    # Update best subset and accuracy if current accuracy is higher
    if (average_accuracy > best_accuracy_DA) {
      best_subset_DA <- predictors_subset_DA
      best_accuracy_DA <- average_accuracy
    }
  }
}

# Print the best subset of predictors and accuracy
print(paste("Best subset of predictors:", paste(best_subset_DA, collapse = ", ")))
print(paste("Best average accuracy:", best_accuracy_DA))
```

I also need to know the null model result for this dataset.

```{r}
# Calculate the frequency of each category in the outcome variable
outcome_frequency <- table(trainData$Obesity)

# Find the most frequent category
most_frequent_category <- names(outcome_frequency)[which.max(outcome_frequency)]

# Create a vector of predicted values with the most frequent category
predicted_outcome <- rep(most_frequent_category, nrow(trainData))

# Calculate the accuracy of the null model
accuracy_null_model <- sum(predicted_outcome == trainData$Obesity) / length(trainData$Obesity)

# Print the accuracy
print(paste("Accuracy of the null model:", accuracy_null_model))
```

Then, I choose another model that can also handle multicategorical outcome named Multinominal Logistic Regression.

```{r}
# Define outcome variable
outcome <- "Obesity"

# Create formula
formula_mlr <- as.formula(paste(outcome, "~", paste(predictors_updated, collapse = " + ")))

# Initialize variables to store results
best_accuracy_mlr <- 0
best_predictors_mlr <- NULL

# Set seed
set.seed(123)

# Perform cross-validation
for (fold in 1:10) {
  # Extract indices for training and validation data for current fold
  train_indices <- unlist(cv_fold1[-fold])
  validation_indices <- cv_fold1[[fold]]
  
  # Fit the Multinomial Logistic Regression model
  model_mlr <- multinom(formula_mlr, data = trainData[train_indices, ])
  
  # Make predictions on the validation set
  predicted_mlr <- predict(model_mlr, newdata = trainData[validation_indices, ], type = "class")
  
  # Calculate accuracy
  accuracy_mlr <- mean(predicted_mlr == trainData$Obesity[validation_indices])
  
  # Update best_accuracy and best_predictors if current accuracy is higher
  if (accuracy_mlr > best_accuracy_mlr) {
    best_accuracy_mlr <- accuracy_mlr
    best_predictors_mlr <- predictors_updated
  }
}

# Print the best predictor combination and highest accuracy
print(paste("Best predictor combination:", paste(best_predictors_mlr, collapse = ", ")))
print(paste("Highest Accuracy:", best_accuracy_mlr))
```

+-----------------------------------+---------------------------------------------------------------+---------------+------------------------------+
| Model                             | Predictor                                                     | Metric Value  | Metric Value on testing data |
+===================================+===============================================================+===============+==============================+
| Linear null                       | \-                                                            | RMSE=8.01     | RMSE=7.99                    |
+-----------------------------------+---------------------------------------------------------------+---------------+------------------------------+
| Linear regression 1               | Age, History, Alcohol, MTRANS                                 | RMSE=0.62     | RMSE=6.54                    |
+-----------------------------------+---------------------------------------------------------------+---------------+------------------------------+
| Linear regression 2               | All                                                           | RMSE=6.29     | RMSE=3.81                    |
+-----------------------------------+---------------------------------------------------------------+---------------+------------------------------+
| Linear regression 2 + LASSO model | All                                                           | RMSE=6.34     | RMSE=6.47                    |
|                                   |                                                               |               |                              |
|                                   |                                                               | l=0.023       |                              |
+-----------------------------------+---------------------------------------------------------------+---------------+------------------------------+
| Random forest                     | All                                                           | RMSE=3.35     | RMSE=3.97                    |
+-----------------------------------+---------------------------------------------------------------+---------------+------------------------------+
| Logistic null                     | \-                                                            | Accuracy=0.17 | Accuracy=0.15                |
+-----------------------------------+---------------------------------------------------------------+---------------+------------------------------+
| Discriminant analysis             | Gender, Age, History, Water, Alcohol, FAF, MTRANS, Gender_Age | Accuracy=0.50 | Accuracy=0.50                |
+-----------------------------------+---------------------------------------------------------------+---------------+------------------------------+
| Multinominal logistic regression  | All                                                           | Accuracy=0.55 | Accuracy=0.50                |
+-----------------------------------+---------------------------------------------------------------+---------------+------------------------------+

Then I will use testData to assess the quality to the model that I choose. Between 2 linear regression models I choose random forest model.

```{r}
# Define formula for random forest
formula_rf_test <- as.formula(paste("BMI ~", paste(predictors_updated, collapse = " + ")))

# Fit random forest model using the entire training data
model_rf_test <- randomForest(formula = formula_rf_test, data = trainData)

# Make predictions on the test data
predictions_rf_test <- predict(model_rf_test, newdata = testData)

# Calculate RMSE on test data
rmse_rf_test <- sqrt(mean((testData$BMI - predictions_rf_test)^2))

# Print RMSE on test data
print(rmse_rf_test)

# Null model of test data
# Compute the mean BMI from the training data
mean_bmi_train <- mean(trainData$BMI)

# Create a vector of predicted values equal to the mean BMI from training data for the test data
predicted_bmi_test <- rep(mean_bmi_train, nrow(testData))

# Calculate RMSE (Root Mean Squared Error) on test data
rmse_null_test <- sqrt(mean((testData$BMI - predicted_bmi_test)^2))

# Print the results
print(paste("Null Model RMSE on Test Data:", rmse_null_test))

# Create a data frame for plotting
plot_data_rf <- data.frame(Observed = testData$BMI, 
                           Predicted_RF = predictions_rf_test,
                           Predicted_Null = predicted_bmi_test)

# Plot using ggplot
p_rf <- ggplot(plot_data_rf, aes(x = Observed, y = Predicted_RF)) +
  geom_point(color = "blue", size = 2, shape = 16) +
  geom_point(aes(y = Predicted_Null), color = "green", size = 2, shape = 16) +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  labs(x = "Observed BMI", y = "Predicted BMI", title = "Observed vs Predicted BMI") +
  theme_minimal() +
  theme(legend.position = "top") +
  scale_shape_manual(values = c(16, 16)) +
  scale_color_manual(values = c("blue", "green")) +
  guides(color = guide_legend(title = "Model"))

plot(p_rf)

# save the plot
figure_file = here("results","figures","Observed-Predicted .png")
ggsave(filename = figure_file, plot=p_rf) 
```

Between 2 logistic models, I choose mlr model.

```{r}
# Define formula for mlr model
formula_mlr_test <- as.formula(paste("Obesity ~", paste(predictors_updated, collapse = " + ")))

# Fit the Multinomial Logistic Regression model using the entire training data
model_mlr_test <- multinom(formula_mlr_test, data = trainData)

# Make predictions on the test data
predicted_mlr_test <- predict(model_mlr_test, newdata = testData, type = "class")

# Calculate accuracy on the test data
accuracy_mlr_test <- mean(predicted_mlr_test == testData$Obesity)

# Print the accuracy
print(paste("Accuracy of the MLR model on test data:", accuracy_mlr_test))
```

Since the neither of the model above has satisfying performance, I will try to assess every model I have used for analysis and check the result.

```{r}
# Compute the mean BMI from the training data
mean_bmi_training <- mean(trainData$BMI)

# Create a vector of predicted values equal to the mean BMI from training data
predicted_bmi_test <- rep(mean_bmi_train, nrow(testData))

# Calculate RMSE on test data
rmse_null_test <- sqrt(mean((testData$BMI - predicted_bmi_test)^2))

# Print the RMSE
print(paste("Null Model RMSE on Test Data:", rmse_null_test))

# Initialize variable to store predictions on the test set
predictions_lm1_test <- rep(NA, nrow(testData))

# Update the formula to include Age, History, Alcohol, and MTRANS as predictors
formula_lm1_test <- paste("BMI ~ Age + History + Alcohol + MTRANS")

# Fit the linear regression model using the updated formula and the training data
model_lm1_test <- lm(formula_lm1_test, data = trainData)

# Make predictions on the test set
predictions_lm1_test <- predict(model_lm1_test, newdata = testData)

# Calculate RMSE on the test data
RMSE_lm1_test <- sqrt(mean((testData$BMI - predictions_lm1_test)^2))

# Print the RMSE on the test data
print(paste("RMSE on test data:", RMSE_lm1_test))

# Define formula for linear model 2
formula_lm2_test <- as.formula(paste("BMI ~", paste(predictors_updated, collapse = " + ")))

# Fit linear model 2 model using the entire training data
model_lm2_test <- randomForest(formula = formula_lm2_test, data = trainData)

# Make predictions on the test data
predictions_lm2_test <- predict(model_lm2_test, newdata = testData)

# Calculate RMSE on test data
rmse_lm2_test <- sqrt(mean((testData$BMI - predictions_lm2_test)^2))

# Print RMSE on test data
print(rmse_lm2_test)

# Fit the LASSO model using the optimal lambda
lasso_model_optimal <- glmnet(as.matrix(trainData1[, predictors_lasso]), trainData1$BMI, alpha = 1, lambda = 0.023)

# Make predictions on the test data using the optimal LASSO model
predictions_lasso_test <- predict(lasso_model_optimal, newx = as.matrix(testData1[, predictors_lasso]))

# Calculate RMSE on the test data
rmse_lasso_test <- sqrt(mean((testData1$BMI - predictions_lasso_test)^2))

# Print RMSE on test data
print(rmse_lasso_test)

# Create a vector of predicted values with the most frequent category for testData
predicted_outcome_test <- rep(most_frequent_category, nrow(testData))

# Calculate the accuracy of the null model on testData
accuracy_null_model_test <- sum(predicted_outcome_test == testData$Obesity)/ length(testData$Obesity)

# Print the accuracy
print(paste("Accuracy of the null model on test data:", accuracy_null_model_test))

# Create new predictors for DA model
predictors_DA <- c("Gender", "Age", "History",  "Water",  "Alcohol", "FAF",  "MTRANS", "Gender_Age")

# Define formula for DA model
formula_DA_test <- as.formula(paste("Obesity ~ ", paste(predictors_DA, collapse = " + ")))

# Fit the DA model using the entire training data
model_DA_test <- lda(formula_DA_test, data = trainData)

# Make predictions on the test data
predicted_DA_test <- predict(model_DA_test, newdata = testData)

# Calculate accuracy on the test data
accuracy_DA_test <- mean(predicted_DA_test$class == testData$Obesity)

# Print the accuracy
print(paste("Accuracy of the DA model on test data:", accuracy_DA_test))
```
I will explore more information for LASSO model.
```{r}
# Extract coefficients from the fitted LASSO model
lasso_coefficients <- coef(lasso_model_optimal)

# Extract non-zero coefficients and their corresponding predictors
non_zero_indices <- which(lasso_coefficients != 0, arr.ind = TRUE)
non_zero_coefficients <- lasso_coefficients[non_zero_indices]

# Extract names of predictors with non-zero coefficients
strongest_predictors <- rownames(lasso_coefficients)[non_zero_indices[, "col"]]

# Find the index of the strongest and weakest coefficients
strongest_index <- which.max(abs(non_zero_coefficients))
weakest_index <- which.min(abs(non_zero_coefficients))

# Extract the strongest and weakest coefficients and their predictors
strongest_coefficient <- non_zero_coefficients[strongest_index]
weakest_coefficient <- non_zero_coefficients[weakest_index]
strongest_predictor <- strongest_predictors[strongest_index]
weakest_predictor <- strongest_predictors[weakest_index]

# Print the results
cat("Strongest predictor:", strongest_predictor, "\n")
cat("Strongest coefficient:", strongest_coefficient, "\n")
cat("Weakest predictor:", weakest_predictor, "\n")
cat("Weakest coefficient:", weakest_coefficient, "\n")
```